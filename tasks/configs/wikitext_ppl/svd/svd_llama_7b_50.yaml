model_args:
  model_type: "llama2"
  model_name: "jeffwan/llama-7b-hf"
  k_name: "self_attn.k_proj"
  q_name: "self_attn.q_proj"
  v_name: "self_attn.v_proj"
  o_name: "self_attn.o_proj"
  up_name: "mlp.up_proj"
  down_name: "mlp.down_proj"
  gate_name: "mlp.gate_proj"
  group_size: 1
  compression_ratio: 50
  context_length: 2048
  stride: 2048
  share_part:
    - "up"
    - "k"
    - "q"
    - "v"
    - "gate"
  private_part:
    - "down"
    - "o"

calibration_args:
  dataset_name: "wikitext"
  build_calib: false
  dataset_cache_dir: null
  calibration_size: 256
  calib_batch_size: 16
  calib_path: "./calib/llama-7b/wikitext/"

after_calibration_update_args:
  build_update_calib: true
  update: true


model_saving:
  update_calib_path: "./calib/llama-7b/wikitext/4update/svd_llama-7b_50"
  save_updated_model: true
  updated_model_path: "./updated_model/svd_llama-7b_50/wikitext"
  save_untrained_model: true
  untrained_model_path: "./untrained_model/svd_llama-7b_50/wikitext"

lora_args:
  save_lora: true
  lora_r: 8
  lora_alpha: 32
  lora_output_dir: "./lora/svd_llama-7b_50/wikitext"
  lora_train_batch_size: 2
  lora_learning_rate: 1.e-4
  lora_train_epoch: 2
  lora_run_name: "svd_llama-7b_50"




